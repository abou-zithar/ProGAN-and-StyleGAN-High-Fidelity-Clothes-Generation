# ProGAN and StyleGAN Implementation with PyTorch

This repository contains implementations of both ProGAN (Progressive Growing of GANs) and StyleGAN using PyTorch. We demonstrate how to integrate ProGAN within the StyleGAN framework in two separate Jupyter notebooks. Additionally, we provide samples of the training data and the generated output data.

## Table of Contents

- [Introduction](#introduction)
- [Usage](#usage)
  - [ProGAN Notebook](#progan-notebook)
  - [StyleGAN Notebook](#stylegan-notebook)
  - [Integration of ProGAN in StyleGAN](#integration-of-progan-in-stylegan)
- [Code Example](#code-example)
- [Data Samples](#data-samples)
  - [Training Data Samples](#training-data-samples)
  - [Output Data Samples](#output-data-samples)
- [Contributing](#contributing)
- [License](#license)

## Introduction

Generative Adversarial Networks (GANs) have revolutionized the field of image generation. ProGAN and StyleGAN are two prominent architectures that have significantly improved the quality and controllability of generated images. This repository explores these architectures and shows how ProGAN can be integrated into StyleGAN for enhanced performance.

## Usage
ProGAN Notebook
The ProGAN implementation can be found in ProGAN.ipynb. To train the ProGAN model, run the notebook cells in sequence:

```bash
jupyter notebook progan-implementation-from-scratch-pytorch.ipynb
```
StyleGAN Notebook
The StyleGAN implementation is available in StyleGAN.ipynb. To train the StyleGAN model, open and run the notebook:

```bash

jupyter notebook StyleGAN implementation from scratch PyTorch.ipynb
```

## Code Example
Here is an example of the Generator class used in the ProGAN implementation:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Generator(nn.Module):
    def __init__(self, z_dim, in_channels, img_channels=3):
        super(Generator, self).__init__()

        self.initial = nn.Sequential(
            PixelNorm(),
            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0),
            nn.LeakyReLU(0.2),
            WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),
            nn.LeakyReLU(0.2),
            PixelNorm(),
        )

        self.initial_rgb = WSConv2d(
            in_channels, img_channels, kernel_size=1, stride=1, padding=0
        )
        self.prog_blocks, self.rgb_layers = (
            nn.ModuleList([]),
            nn.ModuleList([self.initial_rgb]),
        )

        for i in range(len(factors) - 1):  
            conv_in_c = int(in_channels * factors[i])
            conv_out_c = int(in_channels * factors[i + 1])
            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))
            self.rgb_layers.append(
                WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0)
            )

    def fade_in(self, alpha, upscaled, generated):
        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)

    def forward(self, x, alpha, steps):
        out = self.initial(x)

        if steps == 0:
            return self.initial_rgb(out)

        for step in range(steps):
            upscaled = F.interpolate(out, scale_factor=2, mode="nearest")
            out = self.prog_blocks[step](upscaled)

        final_upscaled = self.rgb_layers[steps - 1](upscaled)
        final_out = self.rgb_layers[steps](out)
        return self.fade_in(alpha, final_upscaled, final_out)
```
## Data Samples
Training Data Samples
Here are some samples from the dataset used to train the models. These images are representative of the input data:

![image](https://github.com/abou-zithar/ProGAN-and-StyleGAN-High-Fidelity-Clothes-Generation/assets/43099687/465ff407-cd3f-46f6-9009-07d2d5b146aa)


## Output Data Samples
### Below are samples of images generated by the models after training:
![image](https://github.com/abou-zithar/ProGAN-and-StyleGAN-High-Fidelity-Clothes-Generation/assets/43099687/4347f039-38c1-4c3b-95a8-9ed77286023c)
![image](https://github.com/abou-zithar/ProGAN-and-StyleGAN-High-Fidelity-Clothes-Generation/assets/43099687/c3c5d861-74a4-4421-b7a2-5f5fb0d174a8)
![image](https://github.com/abou-zithar/ProGAN-and-StyleGAN-High-Fidelity-Clothes-Generation/assets/43099687/964b05b7-249e-4eb0-95b7-d35cb966bef3)
![image](https://github.com/abou-zithar/ProGAN-and-StyleGAN-High-Fidelity-Clothes-Generation/assets/43099687/ea976465-92a9-4de4-a460-00532db5e76f)
![image](https://github.com/abou-zithar/ProGAN-and-StyleGAN-High-Fidelity-Clothes-Generation/assets/43099687/b0ea8220-af56-4106-a407-01627e8b7dbf)
![image](https://github.com/abou-zithar/ProGAN-and-StyleGAN-High-Fidelity-Clothes-Generation/assets/43099687/0f1ba0ec-f086-4cc6-b61b-321d72f72a60)


## Contributing
Contributions are welcome! If you have suggestions for improvements or have found bugs, please open an issue or submit a pull request.

## License
This project is licensed under the MIT License. See the LICENSE file for details.
```css

Feel free to modify this `README.md` further to include additional details or sections specific to your project.
```
